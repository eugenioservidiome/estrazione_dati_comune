{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Estrazione Dati Comune - Municipal Data Extraction\n",
        "\n",
        "This notebook extracts missing data from Italian municipality CSV files by:\n",
        "- Crawling official municipality websites\n",
        "- Downloading and analyzing PDF documents  \n",
        "- Using TF-IDF retrieval to find relevant information\n",
        "- Intelligently generating queries and extracting values\n",
        "\n",
        "**Key Features:**\n",
        "- Automatic categorization of missing cells\n",
        "- Multiple queries generated per cell for better coverage\n",
        "- Text extraction from PDF and HTML documents\n",
        "- Document ranking with TF-IDF\n",
        "- Complete audit trail of queries and sources\n",
        "\n",
        "**Version:** 2.0 (Package-based)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies and setup Python path\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src/ to Python path for importing municipality_extractor\n",
        "if 'google.colab' in sys.modules:\n",
        "    repo_path = Path('/content/estrazione_dati_comune')\n",
        "else:\n",
        "    repo_path = Path.cwd()\n",
        "\n",
        "src_path = repo_path / 'src'\n",
        "if src_path.exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "print(f\"✓ Added {src_path} to Python path\")\n",
        "print(\"✓ Dependencies installed successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive (Colab only) - Optional\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully\")\n",
        "    \n",
        "    # Optional: Uncomment to change working directory to Drive folder\n",
        "    # import os\n",
        "    # os.chdir('/content/drive/MyDrive/estrazione_dati_comune')\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Not running in Colab - skipping Drive mount\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure pipeline parameters\n",
        "from municipality_extractor import RunConfig\n",
        "from pathlib import Path\n",
        "\n",
        "config = RunConfig(\n",
        "    # === Required Settings ===\n",
        "    base_url=\"https://www.comune.vigone.to.it/\",  # Municipality website URL\n",
        "    comune=\"Vigone\",  # Municipality name (optional, used in logging)\n",
        "    years_to_fill=[2023, 2024],  # Years to extract data for\n",
        "    \n",
        "    # === Directories ===\n",
        "    input_dir=Path(\"input\"),  # Where to find input CSV files\n",
        "    output_dir=Path(\"output\"),  # Where to save results\n",
        "    # cache_dir will default to output_dir/cache\n",
        "    \n",
        "    # === Crawling Limits ===\n",
        "    max_pages=500,  # Maximum pages to crawl\n",
        "    max_depth=None,  # Maximum crawl depth (None = unlimited)\n",
        "    max_pdf_mb=50.0,  # Maximum PDF file size in MB\n",
        "    \n",
        "    # === Data Sources ===\n",
        "    allow_external_official=False,  # Include external sources (ISTAT, MEF, etc.)\n",
        "    \n",
        "    # === Advanced Settings (usually don't need to change) ===\n",
        "    politeness_delay=0.5,  # Seconds between requests (be polite!)\n",
        "    top_k_queries=10,  # Number of top queries to use per missing cell\n",
        "    max_tfidf_features=5000,  # TF-IDF vectorizer max features\n",
        "    context_window_chars=500,  # Context chars around extracted values\n",
        ")\n",
        "\n",
        "# Display configuration\n",
        "print(\"✓ Configuration created:\")\n",
        "print(f\"  - Base URL: {config.base_url}\")\n",
        "print(f\"  - Municipality: {config.comune}\")\n",
        "print(f\"  - Years: {config.years_to_fill}\")\n",
        "print(f\"  - Input dir: {config.input_dir}\")\n",
        "print(f\"  - Output dir: {config.output_dir}\")\n",
        "print(f\"  - Max pages: {config.max_pages}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run the complete extraction pipeline\n",
        "from municipality_extractor import run_pipeline\n",
        "\n",
        "print(\"Starting extraction pipeline...\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Execute pipeline - this will:\n",
        "# 1. Crawl the municipality website\n",
        "# 2. Extract text from HTML and PDF documents\n",
        "# 3. Build TF-IDF search index\n",
        "# 4. Process all CSV files and fill missing values\n",
        "# 5. Generate audit reports\n",
        "result = run_pipeline(config)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Pipeline complete!\\n\")\n",
        "\n",
        "# Display results summary\n",
        "if result.get('success'):\n",
        "    print(\"✓ Pipeline completed successfully\\n\")\n",
        "    print(f\"Results:\")\n",
        "    print(f\"  - Documents processed: {result.get('documents', 0)}\")\n",
        "    print(f\"  - Values extracted: {result.get('sources', 0)}\")\n",
        "    print(f\"  - Queries generated: {result.get('queries', 0)}\")\n",
        "    print(f\"\\nOutput files saved to: {config.output_dir}/\")\n",
        "    print(f\"  - *_filled.csv: CSV files with extracted values\")\n",
        "    print(f\"  - sources_long.csv: Complete audit trail of sources\")\n",
        "    print(f\"  - queries_generated.csv: All queries used\")\n",
        "    print(f\"  - run_report.md: Summary statistics\")\n",
        "else:\n",
        "    print(f\"✗ Pipeline failed: {result.get('error', 'Unknown error')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: View results and inspect outputs\n",
        "import pandas as pd\n",
        "\n",
        "# List all output files\n",
        "output_files = sorted([f for f in config.output_dir.glob(\"*\") if f.is_file()])\n",
        "print(\"Generated output files:\")\n",
        "for f in output_files:\n",
        "    size_kb = f.stat().st_size / 1024\n",
        "    print(f\"  - {f.name:30s} ({size_kb:>8.1f} KB)\")\n",
        "\n",
        "# Display sources summary if available\n",
        "sources_file = config.output_dir / \"sources_long.csv\"\n",
        "if sources_file.exists():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Sources Summary (first 10 extracted values):\")\n",
        "    print(\"=\" * 80)\n",
        "    sources_df = pd.read_csv(sources_file)\n",
        "    display(sources_df.head(10))\n",
        "    \n",
        "    print(f\"\\nTotal values extracted: {len(sources_df)}\")\n",
        "    print(f\"Average confidence: {sources_df['confidence'].mean():.2%}\")\n",
        "    print(f\"Unique source URLs: {sources_df['source_url'].nunique()}\")\n",
        "\n",
        "# Display extraction report if available\n",
        "report_file = config.output_dir / \"run_report.md\"\n",
        "if report_file.exists():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Extraction Report:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(report_file.read_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Output Files Description\n",
        "\n",
        "The pipeline generates several files in the `output/` directory:\n",
        "\n",
        "### Main Outputs\n",
        "\n",
        "- **`*_filled.csv`** - Your original CSV files with missing values filled in\n",
        "- **`sources_long.csv`** - Complete audit trail showing:\n",
        "  - Which cells were filled (csv_file, row, column)\n",
        "  - Extracted values and confidence scores\n",
        "  - Source URLs and document IDs  \n",
        "  - Text snippets with context\n",
        "  - Keywords that matched\n",
        "\n",
        "### Audit & Analysis Files\n",
        "\n",
        "- **`queries_generated.csv`** - All search queries generated for each missing cell\n",
        "- **`run_report.md`** - Summary statistics including:\n",
        "  - Pages crawled and documents processed\n",
        "  - Fill rates per CSV file\n",
        "  - Success/failure statistics\n",
        "\n",
        "### Cache Directory\n",
        "\n",
        "- **`cache/`** - Downloaded HTML/PDF files and extracted text\n",
        "  - Persists across runs to avoid re-downloading\n",
        "  - Useful for debugging and faster re-runs\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Review filled CSVs** - Check `*_filled.csv` files for accuracy\n",
        "2. **Verify sources** - Inspect `sources_long.csv` to validate data sources\n",
        "3. **Adjust if needed** - Modify configuration parameters and re-run\n",
        "4. **Use cache** - Keep cache/ directory for faster subsequent runs\n",
        "\n",
        "**Tip:** To extract data for a different municipality, simply change `base_url` and `comune` in the configuration cell and re-run cells 4-6.\n",
        "\n",
        "For more information, see the [GitHub repository](https://github.com/eugenioservidiome/estrazione_dati_comune)."
      ]
    }
  ]
}
